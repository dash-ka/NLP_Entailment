{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Textual Entailment Generation system training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, torch\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import warnings, random\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import matplotlib\n",
    "matplotlib.rcParams.update({'figure.figsize': (16, 12), 'font.size': 14})\n",
    "%matplotlib inline\n",
    "\n",
    "from gte_seq2seq import Seq2Seq\n",
    "from utils_model import *\n",
    "from training_gte import *\n",
    "\n",
    "config = {\"BATCH_SIZE\" : 32,\n",
    "          \"HID_DIM\" : 512,\n",
    "          \"N_LAYERS\" : 2,\n",
    "          \"ATTN_TYPE\" : \"luong\",\n",
    "          \"ATTN_FUNC\" : \"dot\",\n",
    "          \"DROPOUT\" : 0.2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load datasets and Build the vocabulary.\n",
    "(specify the folder where the training partition of SNLI dataset is saved) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = EntailmentDataset.load_dataset(r\"D:\\Huawei Share\\Download\", split=\"train\")\n",
    "dev_data = EntailmentDataset.load_dataset(r\"D:\\Huawei Share\\Download\", split=\"dev\")\n",
    "\n",
    "voc = Vocabulary.build_vocabulary(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the pretrained Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "wv = api.load('word2vec-google-news-300')\n",
    "w2v_embeddings, oov = create_w2v_matrix(voc, wv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initiate the Seq2Seq model and choose the loss and the optimizer to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"You are using {device} device\")\n",
    "\n",
    "model = Seq2Seq(pretrained_w2v = w2v_embeddings,\n",
    "                hidden_size = config[\"HID_DIM\"],\n",
    "                n_layers = config[\"N_LAYERS\"],\n",
    "                attn_type = config[\"ATTN_TYPE\"],\n",
    "                attn_func = config[\"ATTN_FUNC\"],\n",
    "                dropout = config[\"DROPOUT\"])\n",
    "\n",
    "model = model.to(device)\n",
    "model.apply(init_weights)\n",
    "print(f'The seq2seq model has {count_parameters(model):,} trainable parameters')\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.NLLLoss(ignore_index = 0)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "train_history = []\n",
    "valid_history = []\n",
    "N_EPOCHS = 1\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss = train(model, voc, train_data, optimizer, criterion, device, train_history, valid_history)\n",
    "    valid_loss = evaluate(model, voc, dev_data, criterion, device)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    train_history.append(train_loss)\n",
    "    valid_history.append(valid_loss)\n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generate inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from display_results import display_attention\n",
    "prem = \"Three puppies are in the tub being sprayed with water by a person.\"\n",
    "hypo, attention = predict(prem, model, device, max_len=20)\n",
    "display_attention(prem, hypo, attention)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
